#Creating m1-m6 with different degrees. Maybe do a loop?
m1 <- lm(Moisture ~ poly(Protein, degree = 1, raw = TRUE), data = train)
m2 <- lm(Moisture ~ poly(Protein, degree = 2, raw = TRUE), data = train)
m3 <- lm(Moisture ~ poly(Protein, degree = 3, raw = TRUE), data = train)
m4 <- lm(Moisture ~ poly(Protein, degree = 4, raw = TRUE), data = train)
m5 <- lm(Moisture ~ poly(Protein, degree = 5, raw = TRUE), data = train)
m6 <- lm(Moisture ~ poly(Protein, degree = 6, raw = TRUE), data = train)
#Calculating MSE for the training data.
mseTrain1 <- mean(m1$residuals^2)
mseTrain2 <- mean(m2$residuals^2)
mseTrain3 <- mean(m3$residuals^2)
mseTrain4 <- mean(m4$residuals^2)
mseTrain5 <- mean(m5$residuals^2)
mseTrain6 <- mean(m6$residuals^2)
mseTrain <- c(mseTrain1, mseTrain2, mseTrain3, mseTrain4, mseTrain5, mseTrain6)
#Predicting with the test data and calculating the MSE.
p1 <- predict(m1, newdata = test, type = "response")
mseTest1 <- mean((p1 - test$Moisture)^2)
p2 <- predict(m2, newdata = test, type = "response")
mseTest2 <- mean((p2 - test$Moisture)^2)
p3 <- predict(m3, newdata = test, type = "response")
mseTest3 <- mean((p3 - test$Moisture)^2)
p4 <- predict(m4, newdata = test, type = "response")
mseTest4 <- mean((p4 - test$Moisture)^2)
p5 <- predict(m5, newdata = test, type = "response")
mseTest5 <- mean((p5 - test$Moisture)^2)
p6 <- predict(m6, newdata = test, type = "response")
mseTest6 <- mean((p6 - test$Moisture)^2)
mseTest <- c(mseTest1, mseTest2, mseTest3, mseTest4, mseTest5, mseTest6)
#Plot booth MSE-arrays.
plot(mseTrain, type="b", col="blue", pch="o", lty=1, xlim=c(0, 7), ylim=c(30, 35))
points(mseTest, type ="b", col="red", pch="*")
#-------------------------------------------------------------------
library("MASS")
channelFat <- data[, 2:102]
model <- lm(Fat ~ ., data = channelFat)
#stepAIC with how many coefficients are chosen.
step <- stepAIC(model, direction="both")
step$anova
summary(step)
step$rank
#-------------------------------------------------------------------
library("glmnet")
#Ridge
covariates=scale(data[, 2:101])
response=scale(data[, 102])
ridge <- glmnet(as.matrix(covariates), response, alpha=0, family="gaussian")
#-------------------------------------------------------------------
#Lasso
lasso <- glmnet(as.matrix(covariates), response, alpha=1, family="gaussian")
#Plot lasso and ridge together.
par(mfrow=c(1,2))
plot(lasso, xvar="lambda", label=TRUE, xlim=c(-7, 0), ylim=c(-3, 5))
plot(ridge, xvar="lambda", label=TRUE, xlim=c(-3, 7), ylim=c(-0.18, 0.35))
#-------------------------------------------------------------------
#Cross-validation
cvLasso <- cv.glmnet(as.matrix(covariates), response, alpha=1, family="gaussian")
cvLasso$lambda.min
plot(cvLasso)
coef(cvLasso, s="lambda.min")
#-------------------------------------------------------------------
library("MASS")
channelFat <- data[, 2:102]
model <- lm(Fat ~ ., data = channelFat)
#stepAIC with how many coefficients are chosen.
step <- stepAIC(model, direction="both")
step$anova
summary(step)
step$rank
#-------------------------------------------------------------------
library("glmnet")
#Ridge
covariates=scale(data[, 2:101])
response=scale(data[, 102])
ridge <- glmnet(as.matrix(covariates), response, alpha=0, family="gaussian")
#-------------------------------------------------------------------
#Lasso
lasso <- glmnet(as.matrix(covariates), response, alpha=1, family="gaussian")
#Plot lasso and ridge together.
par(mfrow=c(1,2))
plot(lasso, xvar="lambda", label=TRUE, xlim=c(-7, 0), ylim=c(-3, 5))
plot(ridge, xvar="lambda", label=TRUE, xlim=c(-3, 7), ylim=c(-0.18, 0.35))
#-------------------------------------------------------------------
#Cross-validation
cvLasso <- cv.glmnet(as.matrix(covariates), response, alpha=1, family="gaussian")
cvLasso$lambda.min
plot(cvLasso)
coef(cvLasso, s="lambda.min")
#-------------------------------------------------------------------
#Cross-validation
lambda = seq(0,3, length = 1000)
cvLasso <- cv.glmnet(as.matrix(covariates), response, lambda=lambda, alpha=1, family="gaussian")
cvLasso$lambda.min
plot(cvLasso)
coef(cvLasso, s="lambda.min")
View(cvLasso)
cvLasso$glmnet.fit
#Lab 1
#Assignment 2
#Loading the data from machines.csv.
data <- read.csv2(file="machines.csv", header=TRUE)
#Function for calculating the loglikelihood.
calculateProbability = function(x, theta) {
n = dim(x)[1]
probability = n*log(theta) - theta*sum(x)
return(probability)
}
#Select x number of values between minData and maxData.
theta = seq(min(data), max(data), length.out = 1000)
#loglikelihood.
logLikelihood = calculateProbability(data, theta)
bestThetaLogLikelihood = theta[which.max(logLikelihood)]
#6 first observations.
firstSix = as.data.frame(data[1:6, ])
logLikelihoodSix = calculateProbability(firstSix, theta)
plot(theta, logLikelihood, type="l", ylim = c(-150, 0), xlab="Theta", ylab="Log Likelihood", main="Log Likelihood and theta", col="red")
lines(theta, logLikelihoodSix, col="blue")
#-------------------------------------------------------------------
#Prior + theta
bayesian <- logLikelihood + log(10) - 10*theta
lines(theta, bayesian, col="green")
bestThetaBayesian <- theta[which.max(bayesian)]
#-------------------------------------------------------------------
set.seed(12345)
newObservations <- rexp(50, rate = bestThetaLogLikelihood)
#Histograms.
par(mfrow=c(1,2))
hist(data$Length, col="red", breaks=15)
hist(newObservations, col="blue", breaks = 15)
par(mfrow=c(1,1))
#Lab 1
#Assignment 2
#Loading the data from machines.csv.
data <- read.csv2(file="machines.csv", header=TRUE)
#Function for calculating the loglikelihood.
calculateProbability = function(x, theta) {
n = dim(x)[1]
probability = n*log(theta) - theta*sum(x)
return(probability)
}
#Select x number of values between minData and maxData.
theta = seq(min(data), max(data), length.out = 1000)
#loglikelihood.
logLikelihood = calculateProbability(data, theta)
bestThetaLogLikelihood = theta[which.max(logLikelihood)]
#6 first observations.
firstSix = as.data.frame(data[1:6, ])
logLikelihoodSix = calculateProbability(firstSix, theta)
plot(theta, logLikelihood, type="l", ylim = c(-150, 0), xlab="Theta", ylab="Log Likelihood", main="Log Likelihood and theta", col="red")
lines(theta, logLikelihoodSix, col="blue")
#-------------------------------------------------------------------
#Prior + theta
bayesian <- logLikelihood + log(10) - 10*theta
lines(theta, bayesian, col="green")
bestThetaBayesian <- theta[which.max(bayesian)]
#-------------------------------------------------------------------
set.seed(12345)
newObservations <- rexp(50, rate = bestThetaLogLikelihood)
#Histograms.
par(mfrow=c(1,2))
hist(data$Length, breaks=15)
hist(newObservations, breaks = 15)
par(mfrow=c(1,1))
#Loading the data from tecator.csv.
data <- read.csv2(file="tecator.csv", header=TRUE)
#Checking if a linear model makes sense.
plot(data$Moisture ~ data$Protein) #Yes
#Dividing data into training and test sets.
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
#Creating m1-m6 with different degrees. Maybe do a loop?
m1 <- lm(Moisture ~ poly(Protein, degree = 1, raw = TRUE), data = train)
train1 <- predict(m1, newdata = poly(train, degree = 1, raw = TRUE), type = "response")
m2 <- lm(Moisture ~ poly(Protein, degree = 2, raw = TRUE), data = train)
train2 <- predict(m1, newdata = poly(train, degree = 2, raw = TRUE), type = "response")
m3 <- lm(Moisture ~ poly(Protein, degree = 3, raw = TRUE), data = train)
train3 <- predict(m1, newdata = poly(train, degree = 3, raw = TRUE), type = "response")
m4 <- lm(Moisture ~ poly(Protein, degree = 4, raw = TRUE), data = train)
train4 <- predict(m1, newdata = poly(train, degree = 4, raw = TRUE), type = "response")
m5 <- lm(Moisture ~ poly(Protein, degree = 5, raw = TRUE), data = train)
train5 <- predict(m1, newdata = poly(train, degree = 5, raw = TRUE), type = "response")
m6 <- lm(Moisture ~ poly(Protein, degree = 6, raw = TRUE), data = train)
train6 <- predict(m1, newdata = poly(train, degree = 6, raw = TRUE), type = "response")
View(train)
train6 <- predict(m1, newdata = poly(train, degree = 6, raw = TRUE), type = "response")
train1 <- predict(m1, newdata = train, type = "response")
#Creating m1-m6 with different degrees. Maybe do a loop?
m1 <- lm(Moisture ~ poly(Protein, degree = 1, raw = TRUE), data = train)
train1 <- predict(m1, newdata = train, type = "response")
m2 <- lm(Moisture ~ poly(Protein, degree = 2, raw = TRUE), data = train)
train2 <- predict(m1, newdata = train, type = "response")
m3 <- lm(Moisture ~ poly(Protein, degree = 3, raw = TRUE), data = train)
train3 <- predict(m1, newdata = train, type = "response")
m4 <- lm(Moisture ~ poly(Protein, degree = 4, raw = TRUE), data = train)
train4 <- predict(m1, newdata = train, type = "response")
m5 <- lm(Moisture ~ poly(Protein, degree = 5, raw = TRUE), data = train)
train5 <- predict(m1, newdata = train, type = "response")
m6 <- lm(Moisture ~ poly(Protein, degree = 6, raw = TRUE), data = train)
train6 <- predict(m1, newdata = train, type = "response")
#Calculating MSE for the training data.
mseTrain1 <- mean((train1 - train$Moisture)^2)
mseTrain2 <- mean((train2 - train$Moisture)^2)
mseTrain3 <- mean((train3 - train$Moisture)^2)
mseTrain4 <- mean((train4 - train$Moisture)^2)
#Creating m1-m6 with different degrees. Maybe do a loop?
m1 <- lm(Moisture ~ poly(Protein, degree = 1, raw = TRUE), data = train)
train1 <- predict(m1, newdata = train, type = "response")
m2 <- lm(Moisture ~ poly(Protein, degree = 2, raw = TRUE), data = train)
train2 <- predict(m2, newdata = train, type = "response")
m3 <- lm(Moisture ~ poly(Protein, degree = 3, raw = TRUE), data = train)
train3 <- predict(m3, newdata = train, type = "response")
m4 <- lm(Moisture ~ poly(Protein, degree = 4, raw = TRUE), data = train)
train4 <- predict(m4, newdata = train, type = "response")
m5 <- lm(Moisture ~ poly(Protein, degree = 5, raw = TRUE), data = train)
train5 <- predict(m5, newdata = train, type = "response")
m6 <- lm(Moisture ~ poly(Protein, degree = 6, raw = TRUE), data = train)
train6 <- predict(m6, newdata = train, type = "response")
#Calculating MSE for the training data.
mseTrain1 <- mean((train1 - train$Moisture)^2)
mseTrain2 <- mean((train2 - train$Moisture)^2)
mseTrain3 <- mean((train3 - train$Moisture)^2)
mseTrain4 <- mean((train4 - train$Moisture)^2)
mseTrain5 <- mean((train5 - train$Moisture)^2)
mseTrain6 <- mean((train6 - train$Moisture)^2)
mseTrain <- c(mseTrain1, mseTrain2, mseTrain3, mseTrain4, mseTrain5, mseTrain6)
#Predicting with the test data and calculating the MSE.
test1 <- predict(m1, newdata = test, type = "response")
mseTest1 <- mean((test1 - test$Moisture)^2)
test2 <- predict(m2, newdata = test, type = "response")
test3 <- predict(m3, newdata = test, type = "response")
mseTest2 <- mean((test2 - test$Moisture)^2)
mseTest3 <- mean((test3 - test$Moisture)^2)
test4 <- predict(m4, newdata = test, type = "response")
mseTest4 <- mean((test4 - test$Moisture)^2)
test5 <- predict(m5, newdata = test, type = "response")
mseTest5 <- mean((test5 - test$Moisture)^2)
test6 <- predict(m6, newdata = test, type = "response")
mseTest6 <- mean((test6 - test$Moisture)^2)
mseTest <- c(mseTest1, mseTest2, mseTest3, mseTest4, mseTest5, mseTest6)
#Plot booth MSE-arrays.
plot(mseTrain, type="b", col="blue", pch="o", lty=1, xlim=c(0, 7), ylim=c(30, 35))
points(mseTest, type ="b", col="red", pch="*")
train1 <- predict(m1, newdata = poly(Protein, degree = 1, raw = TRUE), type = "response")
train1 <- predict(m1, newdata = poly(train, degree = 1, raw = TRUE), type = "response")
step$rank
source('~/R/Lab1/Lab1Assignment4.R')
step$rank
source('~/R/Lab1/Lab1Assignment4.R')
#-------------------------------------------------------------------
#Cross-validation
lambda = seq(0,3, length = 1000)
cvLasso <- cv.glmnet(as.matrix(covariates), response, lambda=lambda, alpha=1, family="gaussian")
cvLasso$lambda.min
plot(cvLasso)
coef(cvLasso, s="lambda.min")
cvLasso$lambda.min
#-------------------------------------------------------------------
#Cross-validation
cvLasso <- cv.glmnet(as.matrix(covariates), response, alpha=1, family="gaussian")
cvLasso$lambda.min
plot(cvLasso)
coef(cvLasso, s="lambda.min")
table(Actual_value = train$Spam, Predicted_value = trainProbability0.9)
View(cvLasso)
install.packages("readxl")
?kernlab
help("ksvm")
?ksvm
?ksvm
library(manipulate)
install.packages(manipulate)
install.packages("manipulate")
library("manipulate")
BetaPlot <- function(a,b){
xGrid <- seq(0.001, 0.999, by=0.001)
prior = dbeta(xGrid, a, b)
maxDensity <- max(prior) # Use to make the y-axis high enough
plot(xGrid, prior, type = 'l', lwd = 3, col = "blue", xlim <- c(0,1), ylim <- c(0, maxDensity), xlab = "theta",
ylab = 'Density', main = 'Beta(a,b) density')
}
manipulate(
BetaPlot(a,b),
a = slider(1, 10, step=1, initial = 2, label = "The hyperparameter a in Beta(a,b) density"),
b = slider(1, 10, step=1, initial = 2, label = "The hyperparameter b in Beta(a,b) density")
)
library("manipulate")
BetaPriorPostPlot <- function(a,b,n,p){
xGrid <- seq(0.001, 0.999, by=0.001)
normalizedLikelihood = dbeta(xGrid, n*p+1, n*(1-p)+1)
prior = dbeta(xGrid, a, b)
posterior = dbeta(xGrid, a+n*p, b+n*(1-p))
maxDensity <- max(normalizedLikelihood, prior, posterior) # Use to make the y-axis high enough
plot(xGrid, normalizedLikelihood, type = 'l', lwd = 3, col = "blue", xlim <- c(0,1), ylim <- c(0, maxDensity), xlab = "theta",
ylab = 'Density', main = 'Bernoulli model - Beta(a,b) prior')
lines(xGrid, posterior, lwd = 3, col = "red")
lines(xGrid, prior, lwd = 3, col = "green")
legend(x = 0.01, y = maxDensity*0.95, legend = c("Likelihood (normalized)", "Prior", "Posterior"), col = c("blue","green","red"), lwd = c(3,3,3), cex = 0.7)
}
manipulate(
BetaPriorPostPlot(a,b,n,p),
a = slider(1, 100, step=1, initial = 2, label = "The hyperparameter a in Beta(a,b) prior"),
b = slider(1, 100, step=1, initial = 2, label = "The hyperparameter b in Beta(a,b) prior"),
n = slider(1, 1000, step=1, initial = 10, label = "The number of trials, n"),
p = slider(0, 1, step=0.01, initial = 0.4, label = "Success proportion in the sample")
)
library(manipulate)
library("manipulate")
NormalPriorPostPlot <- function(mu0,tau0,n,xbar,sigma){
tau02 <- tau0^2
sigma2 <- sigma^2
taun2 <- 1/( n/sigma2 + 1/tau02 )
taun <- sqrt(taun2)
w <- (n/sigma2)/( n/sigma2 + 1/tau02 )
mun <- w*xbar +  (1-w)*mu0
# Finding suitable grid to plot over
IntervalData = c(xbar-4*sigma/sqrt(n),xbar+4*sigma/sqrt(n))
IntervalPrior = c(mu0-4*tau0,mu0+4*tau0)
IntervalPost = c(mun-4*taun,mun+4*taun)
minimum <- min(IntervalData,IntervalPost)
maximum <- max(IntervalData,IntervalPost)
thetaGrid <- seq(minimum, maximum, length=1000)
normalizedLikelihood = dnorm(thetaGrid, mean = xbar, sd = sigma/sqrt(n))
prior = dnorm(thetaGrid, mean = mu0, sd = tau0)
posterior = dnorm(thetaGrid, mean = mun, sd = taun)
maxDensity <- max(normalizedLikelihood, prior, posterior) # Use to make the y-axis high enough
plot(thetaGrid, normalizedLikelihood, type = 'l', lwd = 3, col = "blue", xlim <- c(minimum,maximum), ylim <- c(0, maxDensity), xlab = "theta",
ylab = 'Density', main = expression(paste('Normal model - N(',mu[0],',',tau[0]^2,') prior')))
lines(thetaGrid, posterior, lwd = 3, col = "red")
lines(thetaGrid, prior, lwd = 3, col = "green")
legend(x = minimum, y = maxDensity*0.95, legend = c("Likelihood", "Prior", "Posterior"), col = c("blue","green","red"),
lwd = c(3,3,3), cex = 0.7)
}
manipulate(
NormalPriorPostPlot(mu0,tau0,n,xbar,sigma),
mu0 = slider(-10, 10, step=1, initial = 0, label = "The mean in the Normal(mu0,tau0^2) prior"),
tau0 = slider(0.1, 10, step=.1, initial = 3, label = "The standard deviation in the Normal(mu0,tau0^2) prior"),
n = slider(1, 1000, step=10, initial = 10, label = "The number of data observations, n"),
xbar = slider(-5, 5, step=0.01, initial = 3, label = "Sample mean"),
sigma = slider(0, 100, step=1, initial = 10, label = "Standard deviation of data observations")
)
#------------- Bayesian Learning Code Lecture 1 ----------------#
# Author: Mattias Villani, Statistics, Linkoping University, Sweden. e-mail: mattias.villani@liu.se
library("manipulate")
####################################################################
## Plotting the beta pdf
####################################################################
BetaPlot <- function(a,b){
xGrid <- seq(0.001, 0.999, by=0.001)
prior = dbeta(xGrid, a, b)
maxDensity <- max(prior) # Use to make the y-axis high enough
plot(xGrid, prior, type = 'l', lwd = 3, col = "blue", xlim <- c(0,1), ylim <- c(0, maxDensity), xlab = "theta",
ylab = 'Density', main = 'Beta(a,b) density')
}
manipulate(
BetaPlot(a,b),
a = slider(1, 10, step=1, initial = 2, label = "The hyperparameter a in Beta(a,b) density"),
b = slider(1, 10, step=1, initial = 2, label = "The hyperparameter b in Beta(a,b) density")
)
#----------------------------------------------
# Author: Mattias Villani, Statistics, Linkoping University, Sweden. e-mail: mattias.villani@liu.se
library("manipulate")
####################################################################
## Plotting the prior-to-posterior mapping for the Bernoulli model.
####################################################################
BetaPriorPostPlot <- function(a,b,n,p){
xGrid <- seq(0.001, 0.999, by=0.001)
normalizedLikelihood = dbeta(xGrid, n*p+1, n*(1-p)+1)
prior = dbeta(xGrid, a, b)
posterior = dbeta(xGrid, a+n*p, b+n*(1-p))
maxDensity <- max(normalizedLikelihood, prior, posterior) # Use to make the y-axis high enough
plot(xGrid, normalizedLikelihood, type = 'l', lwd = 3, col = "blue", xlim <- c(0,1), ylim <- c(0, maxDensity), xlab = "theta",
ylab = 'Density', main = 'Bernoulli model - Beta(a,b) prior')
lines(xGrid, posterior, lwd = 3, col = "red")
lines(xGrid, prior, lwd = 3, col = "green")
legend(x = 0.01, y = maxDensity*0.95, legend = c("Likelihood (normalized)", "Prior", "Posterior"), col = c("blue","green","red"), lwd = c(3,3,3), cex = 0.7)
}
manipulate(
BetaPriorPostPlot(a,b,n,p),
a = slider(1, 100, step=1, initial = 2, label = "The hyperparameter a in Beta(a,b) prior"),
b = slider(1, 100, step=1, initial = 2, label = "The hyperparameter b in Beta(a,b) prior"),
n = slider(1, 1000, step=1, initial = 10, label = "The number of trials, n"),
p = slider(0, 1, step=0.01, initial = 0.4, label = "Success proportion in the sample")
)
#-----------------------------------------------
# Author: Mattias Villani, Statistics, Linkoping University, Sweden. e-mail: mattias.villani@liu.se
library("manipulate")
####################################################################
## Plotting the prior-to-posterior mapping for the Normal model.
####################################################################
NormalPriorPostPlot <- function(mu0,tau0,n,xbar,sigma){
tau02 <- tau0^2
sigma2 <- sigma^2
taun2 <- 1/( n/sigma2 + 1/tau02 )
taun <- sqrt(taun2)
w <- (n/sigma2)/( n/sigma2 + 1/tau02 )
mun <- w*xbar +  (1-w)*mu0
# Finding suitable grid to plot over
IntervalData = c(xbar-4*sigma/sqrt(n),xbar+4*sigma/sqrt(n))
IntervalPrior = c(mu0-4*tau0,mu0+4*tau0)
IntervalPost = c(mun-4*taun,mun+4*taun)
minimum <- min(IntervalData,IntervalPost)
maximum <- max(IntervalData,IntervalPost)
thetaGrid <- seq(minimum, maximum, length=1000)
normalizedLikelihood = dnorm(thetaGrid, mean = xbar, sd = sigma/sqrt(n))
prior = dnorm(thetaGrid, mean = mu0, sd = tau0)
posterior = dnorm(thetaGrid, mean = mun, sd = taun)
maxDensity <- max(normalizedLikelihood, prior, posterior) # Use to make the y-axis high enough
plot(thetaGrid, normalizedLikelihood, type = 'l', lwd = 3, col = "blue", xlim <- c(minimum,maximum), ylim <- c(0, maxDensity), xlab = "theta",
ylab = 'Density', main = expression(paste('Normal model - N(',mu[0],',',tau[0]^2,') prior')))
lines(thetaGrid, posterior, lwd = 3, col = "red")
lines(thetaGrid, prior, lwd = 3, col = "green")
legend(x = minimum, y = maxDensity*0.95, legend = c("Likelihood", "Prior", "Posterior"), col = c("blue","green","red"),
lwd = c(3,3,3), cex = 0.7)
}
manipulate(
NormalPriorPostPlot(mu0,tau0,n,xbar,sigma),
mu0 = slider(-10, 10, step=1, initial = 0, label = "The mean in the Normal(mu0,tau0^2) prior"),
tau0 = slider(0.1, 10, step=.1, initial = 3, label = "The standard deviation in the Normal(mu0,tau0^2) prior"),
n = slider(1, 1000, step=10, initial = 10, label = "The number of data observations, n"),
xbar = slider(-5, 5, step=0.01, initial = 3, label = "Sample mean"),
sigma = slider(0, 100, step=1, initial = 10, label = "Standard deviation of data observations")
)
BetaPriorPostPlot <- function(a,b,n,p){
xGrid <- seq(0.001, 0.999, by=0.001)
normalizedLikelihood = dbeta(xGrid, n*p+1, n*(1-p)+1)
prior = dbeta(xGrid, a, b)
posterior = dbeta(xGrid, a+n*p, b+n*(1-p))
maxDensity <- max(normalizedLikelihood, prior, posterior) # Use to make the y-axis high enough
plot(xGrid, normalizedLikelihood, type = 'l', lwd = 3, col = "blue", xlim <- c(0,1), ylim <- c(0, maxDensity), xlab = "theta",
ylab = 'Density', main = 'Bernoulli model - Beta(a,b) prior')
lines(xGrid, posterior, lwd = 3, col = "red")
lines(xGrid, prior, lwd = 3, col = "green")
legend(x = 0.01, y = maxDensity*0.95, legend = c("Likelihood (normalized)", "Prior", "Posterior"), col = c("blue","green","red"), lwd = c(3,3,3), cex = 0.7)
}
manipulate(
BetaPriorPostPlot(a,b,n,p),
a = slider(1, 100, step=1, initial = 2, label = "The hyperparameter a in Beta(a,b) prior"),
b = slider(1, 100, step=1, initial = 2, label = "The hyperparameter b in Beta(a,b) prior"),
n = slider(1, 1000, step=1, initial = 10, label = "The number of trials, n"),
p = slider(0, 1, step=0.01, initial = 0.4, label = "Success proportion in the sample")
)
library("manipulate")
BetaPriorPostPlot <- function(a,b,n,p){
xGrid <- seq(0.001, 0.999, by=0.001)
normalizedLikelihood = dbeta(xGrid, n*p+1, n*(1-p)+1)
prior = dbeta(xGrid, a, b)
posterior = dbeta(xGrid, a+n*p, b+n*(1-p))
maxDensity <- max(normalizedLikelihood, prior, posterior) # Use to make the y-axis high enough
plot(xGrid, normalizedLikelihood, type = 'l', lwd = 3, col = "blue", xlim <- c(0,1), ylim <- c(0, maxDensity), xlab = "theta",
ylab = 'Density', main = 'Bernoulli model - Beta(a,b) prior')
lines(xGrid, posterior, lwd = 3, col = "red")
lines(xGrid, prior, lwd = 3, col = "green")
legend(x = 0.01, y = maxDensity*0.95, legend = c("Likelihood (normalized)", "Prior", "Posterior"), col = c("blue","green","red"), lwd = c(3,3,3), cex = 0.7)
}
manipulate(
BetaPriorPostPlot(a,b,n,p),
a = slider(1, 100, step=1, initial = 2, label = "The hyperparameter a in Beta(a,b) prior"),
b = slider(1, 100, step=1, initial = 2, label = "The hyperparameter b in Beta(a,b) prior"),
n = slider(1, 1000, step=1, initial = 10, label = "The number of trials, n"),
p = slider(0, 1, step=0.01, initial = 0.4, label = "Success proportion in the sample")
)
y = c(14, 25, 45, 25, 30, 33, 19, 50, 34, 67)
mean = 3.5
n = length(y)
n_draws = 10000
# Sample variance
tau_sq = sum((log(y) - mean)^2) / n
# X ~ chi(n)
X_draws = rchisq(n_draws, n)
# This is a draw from inv_chi(n, tausq)
sigma_sq = n * tau_sq / X_draws
# sigma_sq = rinvchisq(n_draws, n, tau_sq)
interval = seq(min(sigma_sq), max(sigma_sq), 0.001)
invchisq = dinvchisq(interval, n, tau_sq)
hist(sigma_sq,freq=F)
y = c(-2.44, 2.14, 2.54, 1.83, 2.02, 2.33, -2.79, 2.23, 2.07, 2.02)
mean = 2.39
k = seq(0.001,10,0.01)
calc_prob = function(k_val, y_val){
prob = exp(k_val * cos(y_val - mean)) / (2*pi*besselI(k_val, 0))
return (prob)
}
calc_post = function(k_val){
probabilities = sapply(y, calc_prob, k_val=k_val)
prior = dexp(k_val)
posterior = prod(probabilities) * prior
return (posterior)
}
posterior = sapply(k, calc_post)
ode = k[which.max(posterior)]
mode = k[which.max(posterior)]
pdf("plots/3_posterior_mode.pdf", width=grid_w, height=grid_h)
plot(k,
posterior, type="l",
col="black",
lwd=2,
xlab="Kappa",
ylab="Posterior",
main="3. Posterior density and mode")
abline(v=mode, col="gray", lwd=2)
# Legend for posterior and mode
legend('topright',
c('Posterior', 'Mode'),
fill=c("black", "gray"),
inset=0.02)
plot(calc_post(k))
sum(posterior)
posterior = sapply(k, calc_post)
sum(posterior)

hist(newObservations, add=T, col=rgb(0, 1, 0, 0.5))
#Histograms.
par(mfrow=c(1,2))
hist(data$Length, xlim=c(0,5), col="red")
hist(newObservations, add=T, col=rgb(0, 1, 0, 0.5))
par(mfrow=c(1,1))
#Histograms.
par(mfrow=c(1,2))
hist(data$Length, xlim=c(0,5), col="red")
hist(newObservations, col=rgb(0, 1, 0, 0.5))
par(mfrow=c(1,1))
#Histograms.
par(mfrow=c(1,2))
hist(data$Length, xlim=c(0,5), col="red", breaks=15)
hist(newObservations, col=rgb(0, 1, 0, 0.5), breaks = 15)
par(mfrow=c(1,1))
#Lab 1
#Assignment 4
#Loading the data from tecator.csv.
data <- read.csv2(file="tecator.csv", header=TRUE)
#Checking if a linear model makes sense.
plot(data$Moisture ~ data$Protein) #Yes
#Dividing data into training and test sets.
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
#Creating m1-m6 with different degrees. Maybe do a loop?
m1 <- lm(Moisture ~ poly(Protein, degree = 1, raw = TRUE), data = train)
m2 <- lm(Moisture ~ poly(Protein, degree = 2, raw = TRUE), data = train)
m3 <- lm(Moisture ~ poly(Protein, degree = 3, raw = TRUE), data = train)
m4 <- lm(Moisture ~ poly(Protein, degree = 4, raw = TRUE), data = train)
m5 <- lm(Moisture ~ poly(Protein, degree = 5, raw = TRUE), data = train)
m6 <- lm(Moisture ~ poly(Protein, degree = 6, raw = TRUE), data = train)
#Calculating MSE for the training data.
mseTrain1 <- mean(m1$residuals^2)
mseTrain2 <- mean(m2$residuals^2)
mseTrain3 <- mean(m3$residuals^2)
mseTrain4 <- mean(m4$residuals^2)
mseTrain5 <- mean(m5$residuals^2)
mseTrain6 <- mean(m6$residuals^2)
mseTrain <- c(mseTrain1, mseTrain2, mseTrain3, mseTrain4, mseTrain5, mseTrain6)
#Predicting with the test data and calculating the MSE.
p1 <- predict(m1, newdata = test, type = "response")
mseTest1 <- mean((p1 - test$Moisture)^2)
p2 <- predict(m2, newdata = test, type = "response")
mseTest2 <- mean((p2 - test$Moisture)^2)
p3 <- predict(m3, newdata = test, type = "response")
mseTest3 <- mean((p3 - test$Moisture)^2)
p4 <- predict(m4, newdata = test, type = "response")
mseTest4 <- mean((p4 - test$Moisture)^2)
p5 <- predict(m5, newdata = test, type = "response")
mseTest5 <- mean((p5 - test$Moisture)^2)
p6 <- predict(m6, newdata = test, type = "response")
mseTest6 <- mean((p6 - test$Moisture)^2)
mseTest <- c(mseTest1, mseTest2, mseTest3, mseTest4, mseTest5, mseTest6)
#Plot booth MSE-arrays.
plot(mseTrain, type="b", col="blue", pch="o", lty=1, xlim=c(0, 7), ylim=c(30, 35))
points(mseTest, type ="b", col="red", pch="*")
#-------------------------------------------------------------------
library("MASS")
channelFat <- data[, 2:102]
model <- lm(Fat ~ ., data = channelFat)
#stepAIC with how many coefficients are chosen.
step <- stepAIC(model, direction="both")
step$anova
summary(step)
step$rank
#-------------------------------------------------------------------
library("glmnet")
#Ridge
covariates=scale(data[, 2:101])
response=scale(data[, 102])
ridge <- glmnet(as.matrix(covariates), response, alpha=0, family="gaussian")
#-------------------------------------------------------------------
#Lasso
lasso <- glmnet(as.matrix(covariates), response, alpha=1, family="gaussian")
#Plot lasso and ridge together.
par(mfrow=c(1,2))
plot(lasso, xvar="lambda", label=TRUE, xlim=c(-7, 0), ylim=c(-3, 5))
plot(ridge, xvar="lambda", label=TRUE, xlim=c(-3, 7), ylim=c(-0.18, 0.35))
#-------------------------------------------------------------------
#Cross-validation
cvLasso <- cv.glmnet(as.matrix(covariates), response, alpha=1, family="gaussian")
cvLasso$lambda.min
plot(cvLasso)
coef(cvLasso, s="lambda.min")
#-------------------------------------------------------------------
library("MASS")
channelFat <- data[, 2:102]
model <- lm(Fat ~ ., data = channelFat)
#stepAIC with how many coefficients are chosen.
step <- stepAIC(model, direction="both")
step$anova
summary(step)
step$rank
#-------------------------------------------------------------------
library("glmnet")
#Ridge
covariates=scale(data[, 2:101])
response=scale(data[, 102])
ridge <- glmnet(as.matrix(covariates), response, alpha=0, family="gaussian")
#-------------------------------------------------------------------
#Lasso
lasso <- glmnet(as.matrix(covariates), response, alpha=1, family="gaussian")
#Plot lasso and ridge together.
par(mfrow=c(1,2))
plot(lasso, xvar="lambda", label=TRUE, xlim=c(-7, 0), ylim=c(-3, 5))
plot(ridge, xvar="lambda", label=TRUE, xlim=c(-3, 7), ylim=c(-0.18, 0.35))
#-------------------------------------------------------------------
#Cross-validation
cvLasso <- cv.glmnet(as.matrix(covariates), response, alpha=1, family="gaussian")
cvLasso$lambda.min
plot(cvLasso)
coef(cvLasso, s="lambda.min")
#-------------------------------------------------------------------
#Cross-validation
lambda = seq(0,3, length = 1000)
cvLasso <- cv.glmnet(as.matrix(covariates), response, lambda=lambda, alpha=1, family="gaussian")
cvLasso$lambda.min
plot(cvLasso)
coef(cvLasso, s="lambda.min")
View(cvLasso)
cvLasso$glmnet.fit
#Lab 1
#Assignment 2
#Loading the data from machines.csv.
data <- read.csv2(file="machines.csv", header=TRUE)
#Function for calculating the loglikelihood.
calculateProbability = function(x, theta) {
n = dim(x)[1]
probability = n*log(theta) - theta*sum(x)
return(probability)
}
#Select x number of values between minData and maxData.
theta = seq(min(data), max(data), length.out = 1000)
#loglikelihood.
logLikelihood = calculateProbability(data, theta)
bestThetaLogLikelihood = theta[which.max(logLikelihood)]
#6 first observations.
firstSix = as.data.frame(data[1:6, ])
logLikelihoodSix = calculateProbability(firstSix, theta)
plot(theta, logLikelihood, type="l", ylim = c(-150, 0), xlab="Theta", ylab="Log Likelihood", main="Log Likelihood and theta", col="red")
lines(theta, logLikelihoodSix, col="blue")
#-------------------------------------------------------------------
#Prior + theta
bayesian <- logLikelihood + log(10) - 10*theta
lines(theta, bayesian, col="green")
bestThetaBayesian <- theta[which.max(bayesian)]
#-------------------------------------------------------------------
set.seed(12345)
newObservations <- rexp(50, rate = bestThetaLogLikelihood)
#Histograms.
par(mfrow=c(1,2))
hist(data$Length, col="red", breaks=15)
hist(newObservations, col="blue", breaks = 15)
par(mfrow=c(1,1))
#Lab 1
#Assignment 2
#Loading the data from machines.csv.
data <- read.csv2(file="machines.csv", header=TRUE)
#Function for calculating the loglikelihood.
calculateProbability = function(x, theta) {
n = dim(x)[1]
probability = n*log(theta) - theta*sum(x)
return(probability)
}
#Select x number of values between minData and maxData.
theta = seq(min(data), max(data), length.out = 1000)
#loglikelihood.
logLikelihood = calculateProbability(data, theta)
bestThetaLogLikelihood = theta[which.max(logLikelihood)]
#6 first observations.
firstSix = as.data.frame(data[1:6, ])
logLikelihoodSix = calculateProbability(firstSix, theta)
plot(theta, logLikelihood, type="l", ylim = c(-150, 0), xlab="Theta", ylab="Log Likelihood", main="Log Likelihood and theta", col="red")
lines(theta, logLikelihoodSix, col="blue")
#-------------------------------------------------------------------
#Prior + theta
bayesian <- logLikelihood + log(10) - 10*theta
lines(theta, bayesian, col="green")
bestThetaBayesian <- theta[which.max(bayesian)]
#-------------------------------------------------------------------
set.seed(12345)
newObservations <- rexp(50, rate = bestThetaLogLikelihood)
#Histograms.
par(mfrow=c(1,2))
hist(data$Length, breaks=15)
hist(newObservations, breaks = 15)
par(mfrow=c(1,1))
#Loading the data from tecator.csv.
data <- read.csv2(file="tecator.csv", header=TRUE)
#Checking if a linear model makes sense.
plot(data$Moisture ~ data$Protein) #Yes
#Dividing data into training and test sets.
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
#Creating m1-m6 with different degrees. Maybe do a loop?
m1 <- lm(Moisture ~ poly(Protein, degree = 1, raw = TRUE), data = train)
train1 <- predict(m1, newdata = poly(train, degree = 1, raw = TRUE), type = "response")
m2 <- lm(Moisture ~ poly(Protein, degree = 2, raw = TRUE), data = train)
train2 <- predict(m1, newdata = poly(train, degree = 2, raw = TRUE), type = "response")
m3 <- lm(Moisture ~ poly(Protein, degree = 3, raw = TRUE), data = train)
train3 <- predict(m1, newdata = poly(train, degree = 3, raw = TRUE), type = "response")
m4 <- lm(Moisture ~ poly(Protein, degree = 4, raw = TRUE), data = train)
train4 <- predict(m1, newdata = poly(train, degree = 4, raw = TRUE), type = "response")
m5 <- lm(Moisture ~ poly(Protein, degree = 5, raw = TRUE), data = train)
train5 <- predict(m1, newdata = poly(train, degree = 5, raw = TRUE), type = "response")
m6 <- lm(Moisture ~ poly(Protein, degree = 6, raw = TRUE), data = train)
train6 <- predict(m1, newdata = poly(train, degree = 6, raw = TRUE), type = "response")
View(train)
train6 <- predict(m1, newdata = poly(train, degree = 6, raw = TRUE), type = "response")
train1 <- predict(m1, newdata = train, type = "response")
#Creating m1-m6 with different degrees. Maybe do a loop?
m1 <- lm(Moisture ~ poly(Protein, degree = 1, raw = TRUE), data = train)
train1 <- predict(m1, newdata = train, type = "response")
m2 <- lm(Moisture ~ poly(Protein, degree = 2, raw = TRUE), data = train)
train2 <- predict(m1, newdata = train, type = "response")
m3 <- lm(Moisture ~ poly(Protein, degree = 3, raw = TRUE), data = train)
train3 <- predict(m1, newdata = train, type = "response")
m4 <- lm(Moisture ~ poly(Protein, degree = 4, raw = TRUE), data = train)
train4 <- predict(m1, newdata = train, type = "response")
m5 <- lm(Moisture ~ poly(Protein, degree = 5, raw = TRUE), data = train)
train5 <- predict(m1, newdata = train, type = "response")
m6 <- lm(Moisture ~ poly(Protein, degree = 6, raw = TRUE), data = train)
train6 <- predict(m1, newdata = train, type = "response")
#Calculating MSE for the training data.
mseTrain1 <- mean((train1 - train$Moisture)^2)
mseTrain2 <- mean((train2 - train$Moisture)^2)
mseTrain3 <- mean((train3 - train$Moisture)^2)
mseTrain4 <- mean((train4 - train$Moisture)^2)
#Creating m1-m6 with different degrees. Maybe do a loop?
m1 <- lm(Moisture ~ poly(Protein, degree = 1, raw = TRUE), data = train)
train1 <- predict(m1, newdata = train, type = "response")
m2 <- lm(Moisture ~ poly(Protein, degree = 2, raw = TRUE), data = train)
train2 <- predict(m2, newdata = train, type = "response")
m3 <- lm(Moisture ~ poly(Protein, degree = 3, raw = TRUE), data = train)
train3 <- predict(m3, newdata = train, type = "response")
m4 <- lm(Moisture ~ poly(Protein, degree = 4, raw = TRUE), data = train)
train4 <- predict(m4, newdata = train, type = "response")
m5 <- lm(Moisture ~ poly(Protein, degree = 5, raw = TRUE), data = train)
train5 <- predict(m5, newdata = train, type = "response")
m6 <- lm(Moisture ~ poly(Protein, degree = 6, raw = TRUE), data = train)
train6 <- predict(m6, newdata = train, type = "response")
#Calculating MSE for the training data.
mseTrain1 <- mean((train1 - train$Moisture)^2)
mseTrain2 <- mean((train2 - train$Moisture)^2)
mseTrain3 <- mean((train3 - train$Moisture)^2)
mseTrain4 <- mean((train4 - train$Moisture)^2)
mseTrain5 <- mean((train5 - train$Moisture)^2)
mseTrain6 <- mean((train6 - train$Moisture)^2)
mseTrain <- c(mseTrain1, mseTrain2, mseTrain3, mseTrain4, mseTrain5, mseTrain6)
#Predicting with the test data and calculating the MSE.
test1 <- predict(m1, newdata = test, type = "response")
mseTest1 <- mean((test1 - test$Moisture)^2)
test2 <- predict(m2, newdata = test, type = "response")
test3 <- predict(m3, newdata = test, type = "response")
mseTest2 <- mean((test2 - test$Moisture)^2)
mseTest3 <- mean((test3 - test$Moisture)^2)
test4 <- predict(m4, newdata = test, type = "response")
mseTest4 <- mean((test4 - test$Moisture)^2)
test5 <- predict(m5, newdata = test, type = "response")
mseTest5 <- mean((test5 - test$Moisture)^2)
test6 <- predict(m6, newdata = test, type = "response")
mseTest6 <- mean((test6 - test$Moisture)^2)
mseTest <- c(mseTest1, mseTest2, mseTest3, mseTest4, mseTest5, mseTest6)
#Plot booth MSE-arrays.
plot(mseTrain, type="b", col="blue", pch="o", lty=1, xlim=c(0, 7), ylim=c(30, 35))
points(mseTest, type ="b", col="red", pch="*")
train1 <- predict(m1, newdata = poly(Protein, degree = 1, raw = TRUE), type = "response")
train1 <- predict(m1, newdata = poly(train, degree = 1, raw = TRUE), type = "response")
step$rank
source('~/R/Lab1/Lab1Assignment4.R')
step$rank
source('~/R/Lab1/Lab1Assignment4.R')
#-------------------------------------------------------------------
#Cross-validation
lambda = seq(0,3, length = 1000)
cvLasso <- cv.glmnet(as.matrix(covariates), response, lambda=lambda, alpha=1, family="gaussian")
cvLasso$lambda.min
plot(cvLasso)
coef(cvLasso, s="lambda.min")
cvLasso$lambda.min
#-------------------------------------------------------------------
#Cross-validation
cvLasso <- cv.glmnet(as.matrix(covariates), response, alpha=1, family="gaussian")
cvLasso$lambda.min
plot(cvLasso)
coef(cvLasso, s="lambda.min")
table(Actual_value = train$Spam, Predicted_value = trainProbability0.9)
View(cvLasso)
install.packages("readxl")
setwd("~/R/Lab2")
source('~/R/Lab2/Lab2Assignment2.R')
trainClassifsWLoss = apply(YfitNaiveTrain, 1, function(row) {
losses = c(1 - row[1], 10*(1 - row[2]))
return(which.min(losses))
})
testClassifsWLoss = apply(YfitNaiveTest, 1, function(row) {
losses = c(1 - row[1], 10*(1 - row[2]))
return(which.min(losses))
})
table(Predicted = trainClassifsWLoss, Actual = train$good_bad)
print("Train misclassification rate: ")
missclass(trainClassifsWLoss, train$good_bad)
table(Predicted = testClassifsWLoss, Actual = test$good_bad)
print("Test misclassification rate: ")
source('~/R/Lab2/Lab2Assignment2.R')
source('~/R/Lab2/Lab2Assignment4.R')
setwd("~/R/Lab2")
source('~/R/Lab2/Lab2Assignment4.R')
library(fastICA)
# a. Compute W' = K * W
set.seed(12345)
fICAResult = fastICA(as.matrix(data), n.comp = 2)
W_prime = fICAResult$K%*%fICAResult$W
plot(W_prime[,1], xlab = "Variable", ylab = "W'")
plot(W_prime[,2], xlab = "Variable", ylab = "W'")
#Loading the data from creditscoring.csv.
data <- read.csv(file="creditscoring.csv", sep=";")
missclass = function(X, X1) {
n=length(X)
return(1-sum(diag(table(X,X1)))/n)
}
# 1.
#Dividing data into training, test and validation sets.
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
id1=setdiff(1:n, id)
set.seed(12345)
id2=sample(id1, floor(n*0.25))
valid=data[id2,]
id3=setdiff(id1,id2)
test=data[id3,]
library("tree")
n=dim(train)[1]
# Deviance
fitDeviance=tree(good_bad ~ ., data=train, split = c("deviance"))
plot(fitDeviance)
text(fitDeviance, pretty=0)
fitDeviance
fitTrainDeviance = predict(fitDeviance, newdata=train, type="class")
table(actual = train$good_bad, predicted = fitTrainDeviance)
summary(fitDeviance)
fitTestDeviance = predict(fitDeviance, newdata=test, type="class")
missclass(train$good_bad, fitTrainDeviance)
table(actual = test$good_bad, predicted = fitTestDeviance)
missclass(test$good_bad, fitTestDeviance)
# Gini index
fitGini=tree(good_bad ~ ., data=train, split = c("gini"))
plot(fitGini)
text(fitGini, pretty=0)
summary(fitGini)
fitTrainGini = predict(fitGini, newdata=train, type="class")
fitGini
missclass(train$good_bad, fitTrainGini)
fitTestGini = predict(fitGini, newdata=test, type="class")
table(actual = test$good_bad, predicted = fitTestGini)
table(actual = train$good_bad, predicted = fitTrainGini)
missclass(test$good_bad, fitTestGini)
fit=tree(good_bad~., data=train, split = c("deviance"))
cv.res=cv.tree(fit)
plot(fit)
text(fit, pretty=0)
par(mfrow=c(1,2))
trainScore = rep(0,9)
testScore = rep(0,9)
for(i in 2:9) {
prunedTree = prune.tree(fit, best=i)
pred = predict(prunedTree, newdata=valid, type="tree")
trainScore[i] = deviance(prunedTree)
testScore[i] = deviance(pred)
}
plot(2:9, trainScore[2:9], type="b", col="red", ylim=c(280,570))
finalTree = prune.tree(fit, best=4)
YfitValid = predict(finalTree, newdata=valid, type="class")
table(valid$good_bad,YfitValid)
points(2:9, testScore[2:9], type="b", col="blue")
plot(finalTree)
YfitTest = predict(finalTree, newdata = test, type="class")
missclass(test$good_bad, YfitTest)
# 4.
library("MASS")
text(finalTree, pretty=0)
library("e1071")
fitNaive = naiveBayes(good_bad ~ ., data=train)
fitNaive
YfitNaiveTrain = predict(fitNaive, newdata = train)
YfitNaiveTest = predict(fitNaive, newdata = test)
table(YfitNaiveTrain, train$good_bad)
missclass(train$good_bad, YfitNaiveTrain)
table(YfitNaiveTest, test$good_bad)
missclass(test$good_bad, YfitNaiveTest)
# 5.
sekvens <- seq(0, 1, 0.05)
predNaiveBayes = predict(fitNaive, newdata=test, type="raw")
predFinalTree = predict(finalTree, newdata=test)
NaiveScore = matrix(NA, nrow = nrow(predNaiveBayes), ncol = length(sekvens))
finalTreeScore = matrix(NA, nrow = nrow(predFinalTree), ncol = length(sekvens))
for(i in 1:length(sekvens)) {
NaiveScore[,i] <- ifelse(predNaiveBayes[,2] > sekvens[i], 1, 0)
finalTreeScore[,i] <- ifelse(predFinalTree[,2] > sekvens[i], 1, 0)
}
# TPR
TPR = function(actual, labels) {
nDataPoints = dim(labels)[1]
nPointsInGraph = dim(labels)[2]
TP = 0
TPR = as.numeric()
for (i in 1:nPointsInGraph) { # for each column
for(j in 1:nDataPoints) { #loop through rows
if(actual[j] == 1 && labels[j,i] == 1) {
TP = TP + 1
}
}
print(TP)
print(sum(actual == 1))
TPR[i] = TP / sum(actual == 1)
TP = 0
}
return(TPR)
}
TPRBayes = TPR(as.numeric(test$good_bad) - 1, NaiveScore)
TPRFinalTree = TPR(as.numeric(test$good_bad) - 1, finalTreeScore)
# FPR
FPR = function(actual, labels) {
nDataPoints = dim(labels)[1]
nPointsInGraph = dim(labels)[2]
FP = 0
FPR = as.numeric()
for (i in 1:nPointsInGraph) { # for each column
for(j in 1:nDataPoints) { #loop through rows
if(actual[j] == 0 && labels[j,i] == 1) {
FP = FP + 1
}
}
print(FP)
print(sum(actual == 0))
FPR[i] = FP / sum(actual == 0)
FP = 0
}
return(FPR)
}
FPRBayes = FPR(as.numeric(test$good_bad) - 1, NaiveScore)
FPRFinalTree = FPR(as.numeric(test$good_bad) - 1, finalTreeScore)
# ROC-curve
plot(x = FPRBayes, y = TPRBayes, type = 'l', col="blue", xlab = 'FPR', ylab = 'TPR', main = 'ROC-curve', xlim = c(0,1), ylim = c(0,1))
lines(x = FPRFinalTree, y = TPRFinalTree, type = 'l', col="red")
trainClassifsWLoss = apply(YfitNaiveTrain, 1, function(row) {
losses = c(1 - row[1], 10*(1 - row[2]))
return(which.min(losses))
})
predictionsTrain = predict(fit, train, "raw")
predictionsTest = predict(fit, test, "raw")
trainClassifsWLoss = apply(predictionsTrain, 1, function(row) {
losses = c(1 - row[1], 10*(1 - row[2]))
return(which.min(losses))
})
testClassifsWLoss = apply(predictionsTest, 1, function(row) {
losses = c(1 - row[1], 10*(1 - row[2]))
# c(bad, good)
return(which.min(losses))
})
print("Training conf matrix rom task 4")
table(train$good_bad, classTrain)
1 - classificationRate(table(train$good_bad, classTrain))
print("Test conf matrix rom task 4")
table(test$good_bad, classTest)
1 - classificationRate(table(test$good_bad, classTest))
table(train$good_bad, trainClassifsWLoss)
1 - classificationRate(table(train$good_bad, trainClassifsWLoss))
table(test$good_bad, testClassifsWLoss)
1 - classificationRate(table(test$good_bad, testClassifsWLoss))
trainClassifsWLoss = apply(YfitNaiveTrain, 1, function(row) {
losses = c(1 - row[1], 10*(1 - row[2]))
return(which.min(losses))
})
model.bayes <- naiveBayes(good_bad~., data=train)
bayes.loss <- predict(model.bayes, newdata=test, type="raw")
bayes.loss
bayes.loss <- ifelse(bayes.loss[,1]*10 > bayes.loss[,2], "bad", "good")
table(test$good_bad, bayes.loss)
misclass.loss_matrix = missclass(test$good_bad, bayes.loss)
misclass.loss_matrix
model.bayes <- naiveBayes(good_bad~., data=test)
bayes.loss <- predict(model.bayes, newdata=test, type="raw")
bayes.loss
bayes.loss <- ifelse(bayes.loss[,1]*10 > bayes.loss[,2], "bad", "good")
table(test$good_bad, bayes.loss)
misclass.loss_matrix = missclass(test$good_bad, bayes.loss)
misclass.loss_matrix
model.bayes <- naiveBayes(good_bad~., data=train)
trainLoss <- predict(model.bayes, newdata=train, type="raw")
trainLoss <- ifelse(trainLoss[,1]*10 > trainLoss[,2], "bad", "good")
table(Predicted = trainLoss, Actual = test$good_bad)
model.bayes <- naiveBayes(good_bad~., data=train)
trainLoss <- predict(model.bayes, newdata=train, type="raw")
trainLoss <- ifelse(trainLoss[,1]*10 > trainLoss[,2], "bad", "good")
table(Predicted = trainLoss, Actual = train$good_bad)
print("Train misclassification rate: ")
missclass(trainLoss, train$good_bad)
testLoss <- predict(model.bayes, newdata=test, type="raw")
testLoss <- ifelse(testLoss[,1]*10 > testLoss[,2], "bad", "good")
table(Predicted = testLoss, Actual = test$good_bad)
print("Test misclassification rate: ")
missclass(testLoss, test$good_bad)
